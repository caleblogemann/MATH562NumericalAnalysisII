\documentclass[11pt]{article}
\usepackage[letterpaper]{geometry}
\usepackage{MATH562}

\begin{document}
\noindent \textbf{\Large{Caleb Logemann \\
MATH 562 Numerical Analysis II \\
Homework 4
}}

%\lstinputlisting[language=Matlab]{H01_23.m}
\begin{enumerate}
    \item % #1
        For each of the following, show that the statement is correct, or give
        a counter-example. 
        If nothing else is written, assume that $A \in \CC^{m \times m}$.
        \begin{enumerate}
            \item % Done
                If $\lambda$ is an eigenvalue of $A$ and $\mu \in \CC$, then
                $\lambda - \mu$ is an eigenvalue of $A - \mu I$.

                Yes this is a true statement.
                \begin{proof}
                    Let $\v{x}$ be the eigenvector for the eigenvalue $\lambda$,
                    that is $A\v{x} = \lambda \v{x}$.
                    Thus
                    \begin{align*}
                        (A - \mu I)\v{x} &= A\v{x} - \mu I \v{x} \\
                                         &= \lambda \v{x} - \mu \v{x} \\
                                         &= (\lambda - \mu) \v{x}
                    \end{align*}
                    Therefore $\v{x}$ is an eigenvector of $A - \mu I$ and the
                    corresponding eigenvalue is $\lambda - \mu$.
                \end{proof}

            \item % Done
                If $A$ is real and $\lambda$ is an eigenvalue of $A$, then
                $-\lambda$ is an eigenvalue of $A$.

                This is false.
                Consider the matrix
                \[
                    A =
                    \begin{bmatrix}
                        2 & 0 \\
                        0 & 3 \\
                    \end{bmatrix}.
                \]
                The eigenvalues of this matrix are $2$ and $3$, neither $-2$
                nor $-3$ are eigenvalues.

            \item
                If $A$ is real and $\lambda$ is an eigenvalue of $A$, then
                $\bar{\lambda}$ is an eigenvalue of $A$.

            \item
                If $\lambda$ is an eigenvalue of $A$ and $A$ is nonsingular, then
                $\lambda^{-1}$ is an eigenvalue of $A^{-1}$.

            \item % Done
                If all the eigenvalues of $A$ are zero, than $A = 0$.

                This is false.
                Consider the matrix
                \[
                    A =
                    \begin{bmatrix}
                        0 & 1 \\
                        0 & 0
                    \end{bmatrix}.
                \]
                Both of the eigenvalues of this matrix are zero, however
                $A \neq 0$.

            \item
                If $A$ is Hermitian and $\lambda$ is an eigenvalue of $A$, then
                $\lambda$ is a singular value of $A$.

            \item
                If $A$ is diagonalizable and all eigenvalues are equal, then $A$
                is diagonal.
        \end{enumerate}

    \item % #2
        \begin{enumerate}
            \item[(a)]
                Let $A \in \CC^{m \times m}$ be tridiagonal and Hermitian, with
                all of its subdiagonal and superdiagonal entries nonzero.
                Prove that the eigenvalues of $A$ are distinct.

                % Hint show that for any $\lambda \in \CC$, $A - \lambda I$ has
                % rank at least m - 1.

            \item[(b)]
                Let $A$ be upper-Hessenberg, with all of its subdiagonal entries
                nonzero.
                Give an example that shows that the eigenvalues of $A$ are
                not necessarily distinct.
        \end{enumerate}

    \item % #3
        Suppose $A$ is $m \times m$ and has a complete set of orthonormal
        eigenvectors, $\v{q}_1, \ldots, \v{q}_m$, and with corresponding
        eigenvalues $\lambda_1, \ldots, \lambda_m$.
        Assume that the ordering is such that
        $\abs{\lambda_j} \ge \abs{\lambda_{j+1}}$.
        Furthermore assume that
        $\abs{\lambda_1} > \abs{\lambda_2} > \abs{\lambda_3}$.
        Consider the artificial version of the power method
        $\v{v}^{(k)} = A\v{v}^{(k-1)}/\lambda_1$ with
        $\v{v}^{(0)} = \alpha_1 \v{q}_1 + \cdots + \alpha_m \v{q}_m$, where
        $\alpha_1$ and $\alpha_2$ are both nonzero.
        Show that the sequence converges linearly to $\alpha_1 \v{q}_1$ with
        asymptotic constant $C = \abs{\lambda_2/\lambda_1}$.

        \begin{proof}
            
        \end{proof}

    \item % #4 Done
        Consider the matrix
        \[
            A =
            \begin{bmatrix}
                -1 &  0 &  1 \\
                 1 & -1 &  0 \\
                 0 &  1 & -1 \\
                 1 &  0 &  1
            \end{bmatrix}
        \]
        \begin{enumerate}
            \item[(a)] % Done
                Calculate the eigenvalues and eigenvectors of $A^T A$

                First we must compute the matrix, $A^T A$.
                \[
                    A^T A =
                    \begin{bmatrix}
                        3 & -1 & 0 \\
                        -1 & 2 & -1 \\
                        0 & -1 & 3
                    \end{bmatrix}
                \]
                The eigenvalues can be found by using the characteristic
                polynomial, that is $p(z) = \det(zI - A^T A)$.
                \begin{align*}
                    \det(zI - A^T A) &=
                    \begin{vmatrix}
                        z - 3 &    1 &     0 \\
                           1 & z - 2 &    1 \\
                            0 &    1 & z - 3
                    \end{vmatrix} \\
                    &= (z - 3)^2(z - 2) - (z - 3) - (z - 3) \\
                    &= (z - 3)((z - 3)(z - 2) - 2) \\
                    &= (z - 3)\p{z^2 - 5z + 4} \\
                    &= (z - 3)(z - 4)(z - 1)
                \end{align*}
                The eigenvalues are the zeros of the characteristic polynomial,
                therefore $\spec(A) = \set{1, 3, 4}$.

                The eigenvectors of $A^T A$ can be found by solving the following systems
                \begin{align*}
                    (I - A^T A)\v{x} &= \v{0} \\
                    (3I - A^T A)\v{x} &= \v{0} \\
                    (4I - A^T A)\v{x} &= \v{0} \\
                \end{align*}

                First I will solve $(I - A^T A)\v{x} = \v{0}$ using the
                augmented system.
                \begin{align*}
                    \begin{bmatrix}
                        -2 &  1 &  0 & 0 \\
                         1 & -1 &  1 & 0 \\
                         0 &  1 & -2 & 0
                    \end{bmatrix} \\
                    \begin{bmatrix}
                         1 & -1/2 &  0 & 0 \\
                         1 & -1 &  1 & 0 \\
                         0 &  1 & -2 & 0
                    \end{bmatrix} \\
                    \begin{bmatrix}
                         1 & -1/2 &  0 & 0 \\
                         0 & -1/2 &  1 & 0 \\
                         0 &  1 & -2 & 0
                    \end{bmatrix} \\
                    \begin{bmatrix}
                         1 & -1/2 & 0 & 0 \\
                         0 & 1 & -2 & 0 \\
                         0 & 1 & -2 & 0
                    \end{bmatrix} \\
                    \begin{bmatrix}
                         1 & -1/2 &  0 & 0 \\
                         0 & 1 & -2 & 0 \\
                         0 & 0 & 0 & 0
                    \end{bmatrix} \\
                    \begin{bmatrix}
                         1 & 0 & -1 & 0 \\
                         0 & 1 & -2 & 0 \\
                         0 & 0 &  0 & 0
                    \end{bmatrix} \\
                \end{align*}
                Thus the solutions to this system are of the form
                \begin{align*}
                    \begin{bmatrix}
                        x \\
                        2x \\
                        x
                    \end{bmatrix}
                \end{align*}
                The eigenvector with 2-norm equal to one for eigenvalue 1 is
                \begin{align*}
                    \begin{bmatrix}
                        1/\sqrt{6} \\
                        2/\sqrt{6} \\
                        1/\sqrt{6}
                    \end{bmatrix}
                \end{align*}

                The eigenvector vector for eigenvalue 3 can be found as
                \begin{align*}
                    \begin{bmatrix}
                        0 & 1 & 0 & 0 \\
                        1 & 1 & 1 & 0 \\
                        0 & 1 & 0 & 0
                    \end{bmatrix} \\
                    \begin{bmatrix}
                        1 & 1 & 1 & 0 \\
                        0 & 1 & 0 & 0 \\
                        0 & 1 & 0 & 0
                    \end{bmatrix} \\
                    \begin{bmatrix}
                        1 & 1 & 1 & 0 \\
                        0 & 1 & 0 & 0 \\
                        0 & 0 & 0 & 0
                    \end{bmatrix} \\
                    \begin{bmatrix}
                        1 & 0 & 1 & 0 \\
                        0 & 1 & 0 & 0 \\
                        0 & 0 & 0 & 0
                    \end{bmatrix} \\
                \end{align*}
                Thus the solutions to this system are of the form
                \begin{align*}
                    \begin{bmatrix}
                        x \\
                        0 \\
                        -x
                    \end{bmatrix}
                \end{align*}
                The eigenvector with 2-norm equal to one for eigenvalue 3 is
                \begin{align*}
                    \begin{bmatrix}
                        1/\sqrt{2} \\
                        0 \\
                        -1/\sqrt{2}
                    \end{bmatrix}
                \end{align*}

                Lastly the eigenvector for eigenvalue 4 is needed.
                \begin{align*}
                    \begin{bmatrix}
                        1 & 1 & 0 & 0 \\
                        1 & 2 & 1 & 0 \\
                        0 & 1 & 1 & 0
                    \end{bmatrix} \\
                    \begin{bmatrix}
                        1 & 1 & 0 & 0 \\
                        0 & 1 & 1 & 0 \\
                        0 & 1 & 1 & 0
                    \end{bmatrix} \\
                    \begin{bmatrix}
                        1 & 1 & 0 & 0 \\
                        0 & 1 & 1 & 0 \\
                        0 & 0 & 0 & 0
                    \end{bmatrix} \\
                    \begin{bmatrix}
                        1 & 0 & -1 & 0 \\
                        0 & 1 & 1 & 0 \\
                        0 & 0 & 0 & 0
                    \end{bmatrix} \\
                \end{align*}
                Thus the solutions to this system are of the form
                \begin{align*}
                    \begin{bmatrix}
                        x \\
                        -x \\
                        x
                    \end{bmatrix}
                \end{align*}
                The eigenvector with 2-norm equal to one for eigenvalue 3 is
                \begin{align*}
                    \begin{bmatrix}
                        1/\sqrt{3} \\
                        -1/\sqrt{3} \\
                        1/\sqrt{3}
                    \end{bmatrix}
                \end{align*}

                Thus the eigenvalue decomposition of $A^T A$ is
                \begin{align*}
                    A^T A &= X \Lambda X' \\
                    X &=
                    \begin{bmatrix}
                        1/\sqrt{6} & 1/\sqrt{2}  & 1/\sqrt{3} \\
                        2/\sqrt{6} & 0           & -1/\sqrt{3} \\
                        1/\sqrt{6} & -1/\sqrt{2} & 1/\sqrt{3}
                    \end{bmatrix} \\
                    \Lambda &=
                    \begin{bmatrix}
                        1 & 0 & 0 \\
                        0 & 3 & 0 \\
                        0 & 0 & 4
                    \end{bmatrix}
                \end{align*}

            \item[(b)] % Done
                Use your results in (a) to compute (by hand) the SVD of $A$.

                The singular values of $A$ are the nonnegative square roots of
                the eigenvalues of $A^T A$.
                Thus if $A = U \Sigma V^T$ is a singular value decomposition of
                $A$, then
                \begin{align*}
                    \Sigma &=
                    \begin{bmatrix}
                        1 & 0 & 0 \\
                        0 & \sqrt{3} & 0 \\
                        0 & 0 & 2
                    \end{bmatrix} \\
                    V &=
                    \begin{bmatrix}
                        1/\sqrt{6} & 1/\sqrt{2}  & 1/\sqrt{3} \\
                        2/\sqrt{6} & 0           & -1/\sqrt{3} \\
                        1/\sqrt{6} & -1/\sqrt{2} & 1/\sqrt{3}
                    \end{bmatrix} \\
                \end{align*}
                The unitary matrix $U$ can be found by solving the system
                $U\Sigma = AV$, for $U$ unitary.
                Since $\Sigma$ is invertible, $U = AV\Sigma^{-1}$
                \begin{align*}
                    U &=
                    \begin{bmatrix}
                        0 & -2/\sqrt{6} & 0 \\
                        -1/\sqrt{6} & 1/\sqrt{6} & 1/\sqrt{3} \\
                        1/\sqrt{6} & 1/\sqrt{6} & -1/\sqrt{3} \\
                        2/\sqrt{6} & 0 & 1/\sqrt{3} \\
                    \end{bmatrix}
                \end{align*}

            \item[(c)] % Done
                Find the $1$-, $2$-, $\infty$-, and Frobenius norms of $A$.

                The $1$-norm of a matrix is the maximum absolute column sum.
                Therefore $\norm[1]{A} = 3$.

                The $2$-norm of a matrix is the maximum singular value.
                Therefore $\norm[2]{A} = 2$.

                The $\infty$-norm of a matrix is the maximum absolute row sum.
                Therefore $\norm[\infty]{A} = 2$.

                The Frobenius norm of a matrix is the squareroot of the sum of
                the squares of the entries.
                Therefore $\norm[F]{A} = \sqrt{8} = 2\sqrt{2}$.
        \end{enumerate}

    \item % #5 Done
        Write a MATLAB function $[v, lam, k] = Pwr(A, v0)$ that uses the method
        of power iteration to compute the largest eigenvalue, ``$lam$'', and a
        corresponding eigenvector $v$ that has length one in the 2-norm.
        The third argument returned, $k$, should be the number of iterations used in
        the computation.
        The input data is a square matrix $A$ and a starting vector $v0$.

        \lstinputlisting[language=Matlab]{Pwr1.m}
        \lstinputlisting[language=Matlab]{Pwr2.m}
        \lstinputlisting[language=Matlab, lastline=12]{H04.m}

        For the first data set, the second criterion worked much better.
        In fact the first criterion didn't even converge fully.
        The first criterion stopped at 500 iterations.
        \begin{verbatim}
v1 =

    1.0000
    0.0000
    0.0000
    0.0000
    0.0000


lam1 =

    -4


k1 =

   500


v2 =

    1.0000
    0.0000
    0.0000
    0.0000
    0.0000


lam2 =

   -4.0000


k2 =

    28
    \end{verbatim}
    For the second data set the second criterion worked only slightly better.
    This may be because the eigenvalues are more spread out in the second
    data set.
    \begin{verbatim}
v1 =

    1.0000
    0.0000
    0.0000
    0.0000
   -0.0000


lam1 =

    9.0000


k1 =

   161


v2 =

    1.0000
   -0.0000
   -0.0000
   -0.0000
    0.0000


lam2 =

    9.0000


k2 =

   158
        \end{verbatim}

    \item % #6 Done
        The function for the Inverse iteration is shown below
        \lstinputlisting[language=Matlab]{Inv.m}
        \lstinputlisting[language=Matlab, firstline=14, lastline=17]{H04.m}

        This method converged extremely fast, much faster than the power
        iteration.
        I think that each iteration may be more work, than an iteration of the
        Power iteration, because the Inverse iteration requires solving a linear
        system.
        That being said the additional knowledge of approximately the eigenvalue
        allows for faster convergence.
        \begin{verbatim}
v =

    1.0000
    0.0000
    0.0000
    0.0000
    0.0000


lam =

    9.0000


k =

     8
        \end{verbatim}

    \item % #7 Done
        The function for the Rayleigh quotient iteration is shown below
        \lstinputlisting[language=Matlab]{Ray.m}
        \lstinputlisting[language=Matlab, firstline=19]{H04.m}

        The Rayleigh quotient iteration is the most efficient
        algorithm so far and it is able to find any eigenvalue
        eigenvector pair.
        This algorithm will locate whatever eigenvalue whose eigenvector
        is closest to the initial input vector.
        Trying all sorts of different input vectors results in different
        eigenvalue/eigenvector pairs being found.
        Each time the algorithm converges extremely quickly.
        The number of iterations is always in the single digits.
        Below is a sample result.
        \begin{verbatim}
v =

   -0.0482
    0.9988
    0.0000
    0.0000
    0.0000


lam =

    2.0000


k =

     6
        \end{verbatim}

\end{enumerate}
\end{document}
