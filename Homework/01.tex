\documentclass[11pt]{article}
\usepackage[letterpaper]{geometry}
\usepackage{MATH562}

\begin{document}
\noindent \textbf{\Large{Caleb Logemann \\
MATH 562 Numerical Analysis II \\
Homework 1
}}

\begin{enumerate}
    \item % #1
        Problem 1.1
        Let $B$ be a $4 \times 4$ matrix to which the following operations are
        applied in the given order.
        \begin{enumerate}
            \item[1.] double column 1
            \item[2.] halve row 3
            \item[3.] add row 3 to row 1
            \item[4.] interchange columns 1 and 4
            \item[5.] subtract row 2 from each other rows
            \item[6.] replace column 4 by column 3
            \item[7.] delete column 1
        \end{enumerate}
        The result can be written as a product of 8 matrices one of which is $B$.
        \begin{enumerate}
            \item[(a)]
                What are the other 7 matrices and what order do they appear in
                the matrix?

                The matrix that doubles column 1 is
                \begin{align*}
                    C &=
                    \begin{bmatrix}
                        2 & 0 & 0 & 0 \\
                        0 & 1 & 0 & 0 \\
                        0 & 0 & 1 & 0 \\
                        0 & 0 & 0 & 1
                    \end{bmatrix}
                    \intertext{when right multiplied.
                        The following matrix halves row 3 when left multiplied.}
                    D &=
                    \begin{bmatrix}
                        1 & 0 & 0 & 0 \\
                        0 & 1 & 0 & 0 \\
                        0 & 0 & .5 & 0 \\
                        0 & 0 & 0 & 1
                    \end{bmatrix}
                    \intertext{The following matrix adds row 3 to the row 1 when left multiplied.}
                    E &= 
                    \begin{bmatrix}
                        1 & 0 & 1 & 0 \\
                        0 & 1 & 0 & 0 \\
                        0 & 0 & 1 & 0 \\
                        0 & 0 & 0 & 1
                    \end{bmatrix}
                    \intertext{The following matrix interchanges columns 1 and 4 when right multiplied.}
                    F &=
                    \begin{bmatrix}
                        0 & 0 & 0 & 1 \\
                        0 & 1 & 0 & 0 \\
                        0 & 0 & 1 & 0 \\
                        1 & 0 & 0 & 0
                    \end{bmatrix}
                    \intertext{The following matrix subtracts row 2 from every other row, when left multiplied.}
                    G &=
                    \begin{bmatrix}
                        1 & -1 & 0 & 0 \\
                        0 & 1 & 0 & 0 \\
                        0 & -1 & 1 & 0 \\
                        0 & -1 & 0 & 1
                    \end{bmatrix}
                    \intertext{The following matrix replaces column 4 with column 3 when right multiplied.}
                    H &=
                    \begin{bmatrix}
                        1 & 0 & 0 & 0 \\
                        0 & 1 & 0 & 0 \\
                        0 & 0 & 1 & 1 \\
                        0 & 0 & 0 & 0
                    \end{bmatrix}
                    \intertext{The following matrix deletes column 1 when right multiplied.}
                    I &=
                    \begin{bmatrix}
                        0 & 0 & 0 \\
                        1 & 0 & 0 \\
                        0 & 1 & 0 \\
                        0 & 0 & 1
                    \end{bmatrix}
                \end{align*}
                The resulting matrix product is given by $GEDBCFHI$, where the
                matrices are given above.

            \item[(b)]
                The result can also be written as a product $ABC$ what are $A$
                and $C$?

                In this case $A$ and $C$ are given by the product of the
                matrices to the left and the right of $B$ in the part (a).
                Therefore
                \begin{align*}
                    A &=
                    \begin{bmatrix}
                        1 & -1 & 0.5 & 0 \\
                        0 &  1 & 0 & 0 \\
                        0 & -1 & 0.5 & 0 \\
                        0 & -1 & 0 & 1
                    \end{bmatrix} \\
                    C &=
                    \begin{bmatrix}
                        0 & 0 & 0 \\
                        1 & 0 & 0 \\
                        0 & 1 & 1 \\
                        0 & 0 & 0
                    \end{bmatrix}
                \end{align*}

        \end{enumerate}

    \item % #2

    \item % #3 Problem 2.3 page 15
        Let $A \in \CC^{m \times m}$ be Hermitian, that is $A = A^*$.
        Suppose that $A\v{x} = \lambda\v{x}$, where $\v{x} \in \CC^{m \times m}$
        and $\lambda \in \CC$, so $\v{x}$ is an eigenvector and $\lambda$ is an
        eigenvalue.
        \begin{enumerate}
            \item[(a)]
                Prove that $\lambda$ must be real.

                \begin{proof}
                    Consider $\v{x}^* A \v{x}$.
                    \begin{align*}
                        \p{\v{x}^* A} \v{x} &= \v{x}^* \p{A \v{x}} \\
                        \p{A^* \v{x}}^* \v{x} &= \v{x}^* \p{A \v{x}} \\
                        \p{\lambda \v{x}}^* \v{x} &= \v{x}^* \p{\lambda \v{x}} \\
                        \overline{\lambda} \v{x}^* \v{x} &= \lambda \v{x}^*\v{x}  \\
                        \overline{\lambda} &= \lambda
                    \end{align*}
                    Since $\overline{\lambda} = \lambda$, $\lambda$ must be real. 
                \end{proof}

            \item[(b)]
                Prove that if $\v{x}$ and $\v{y}$ are eigenvectors
                corresponding to different eigenvalues, then $\v{x}$ and
                $\v{y}$ are orthogonal.

                \begin{proof}
                    Let $\lambda_x$ and $\lambda_y$ be the eigenvalues corresponding
                    to $\v{x}$ and $\v{y}$ respectively, that is
                    $A\v{x} = \lambda_x\v{x}$, $A\v{y} = \lambda_y\v{y}$, and
                    $\lambda_x \neq \lambda_y$.
                    Since $A$ is Hermitian, we have just shown that
                    $\lambda_x, \lambda_y \in \RR$.
                    Also because $A$ is Hermitian
                    \begin{align*}
                        A &= A^* \\
                        \v{x}^* A &= \v{x}^* A^* \\
                        \v{x}^* A &= \p{A\v{x}}^*.
                        \intertext{Since $\v{x}$ is an eigenvector}
                        \v{x}^* A &= \p{\lambda_x \v{x}}^*
                        \intertext{Since $\lambda_x \in \RR$}
                        \v{x}^* A &= \lambda_x \v{x}^* \\
                        \v{x}^* A \v{y} &= \lambda_x \v{x}^* \v{y} \\
                        \v{x}^* \lambda_y \v{y} &= \lambda_x \v{x}^* \v{y} \\
                        \p{\lambda_y - \lambda_x} \v{x}^* \v{y} &= 0
                        \intertext{Since $\lambda_x \neq \lambda_y$, $\lambda_y - \lambda_x \neq 0$.}
                        \v{x}^* \v{y} &= 0
                    \end{align*}
                    Therefore $\v{x}$ and $\v{y}$ are orthogonal.
                \end{proof}

        \end{enumerate}

    \item % #4 Problem 2.6 page 16
        If $\v{u}, \v{v} \in \RR^m$, then $A = I + \v{u}\v{v}^T$ is called a
        rank one perturbation of the identity.
        \begin{enumerate}
            \item[(a)]
                Show that is $A$ is invertible, then it's inverse has the form
                $A^{-1} = I + \alpha \v{u}\v{v}^T$, and give an expression for
                the scalar $\alpha$.



            \item[(b)]
                If $A$ is not invertible than $\v{v}^T \v{u} = -1$, otherwise
                $I + \alpha \v{u}\v{v}^T$, where
                $\alpha = -\frac{1}{1 + \v{v}^T\v{u}}$ would be an inverse.
                In this case $\null{A}$
        \end{enumerate}

    \item % #5 Problem 3.2 page 24 Done
        Let $\norm{\cdot}$ denote any norm on $\CC^m$ and also the corresponding
        induced norm on $\CC^{m \times m}$, so that
        $\norm{A\v{x}} \le \norm{A}\norm{\v{x}}$.
        Show that $\rho(A) \le \norm{A}$, where
        $\rho(A) = \max{\abs{\lambda}:\text{where $\lambda$ is an eigenvalue of $A$}}$
        is the spectral radius of $A$.

        \begin{proof}
            Let $\v{x}$ be an eigenvector of $A$ with corresponding eigenvalue
            $\lambda$, such that $\abs{\lambda} = \rho(A)$, that is $\v{x}$ is
            the eigenvector corresponding to the eigenvalue whose magnitude is
            the spectral radius.
            Consider $\norm{A\v{x}}$
            \begin{align*}
                \norm{A\v{x}} &= \norm{\lambda \v{x}} \\
                &= \abs{\lambda} \norm{\v{x}} \\
                &= \rho(A) \norm{\v{x}}
            \end{align*}
            Also note that $\norm{A\v{x}} \le \norm{A}\norm{\v{x}}$, therefore
            \begin{align*}
                \rho(A) \norm{\v{x}} &\le \norm{A}\norm{\v{x}} \\
                \rho(A) &\le \norm{A}
            \end{align*}
            Since $\norm{\v{x}} \neq 0$, because $\v{x}$ is an eigenvector.
        \end{proof}

    \item % #6
        Let $\theta \in \p{0, 2\pi}$ and 
    \item % #7
\end{enumerate}
\end{document}
