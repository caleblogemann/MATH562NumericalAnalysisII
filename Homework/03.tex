\documentclass[11pt]{article}
\usepackage[letterpaper]{geometry}
\usepackage{MATH562}

\begin{document}
\noindent \textbf{\Large{Caleb Logemann \\
MATH 562 Numerical Analysis II \\
Homework 3
}}

%\lstinputlisting[language=Matlab]{H01_23.m}
\begin{enumerate}
    \item % #1 Done
        Determine the relative condition number for the following problem.
        Are there values of $x$ for which the problem is ill-conditioned?
        Justify your answer.
        \[
            f(x) = \frac{1 - e^{-x}}{1 + e^{-x}}
        \]

        Since $f$ is differentiable the relative condition number of $f$ is given by
        $\kappa = \frac{\abs{f'(x)}}{\abs{f(x)}/\abs{x}}$.
        For this problem
        \begin{align*}
            f'(x) &= \frac{\p{1 + e^{-x}}e^{-x} - \p{1 - e^{-x}}\p{-e^{-x}}}{\p{1 + e^{-x}}^2} \\
                  &= \frac{e^{-x} + e^{-2x} + e^{-x} - e^{-2x}}{\p{1 + e^{-x}}^2} \\ 
                  &= \frac{2e^{-x}}{\p{1 + e^{-x}}^2} \\ 
        \end{align*}
        Thus the relative condition number for this problem is
        \begin{align*}
            \kappa &= \frac{\abs{f'(x)}}{\abs{f(x)}/\abs{x}} \\
                   &= \abs{\frac{2xe^{-x}}{\p{1 + e^{-x}}^2} / \frac{1 - e^{-x}}{1 + e^{-x}}} \\
                   &= \abs{\frac{2xe^{-x}}{\p{1 + e^{-x}}^2} \times \frac{1 + e^{-x}}{1 - e^{-x}}} \\
                   &= \abs{\frac{2xe^{-x}}{\p{1 + e^{-x}}} \times \frac{1}{1 - e^{-x}}} \\
                   &= \abs{\frac{2xe^{-x}}{\p{1 - e^{-2x}}}} \\
        \end{align*}

        This problem is not ill-conditioned because for any $x$ this relitive
        condition number is small.
        At $x = 0$, this condition number is undefined, but L'Hopital's rule
        shows that the limit is equal to $1$.
        \begin{align*}
            \lim{x \to 0}{\kappa} &= \lim{x \to 0}{\frac{2 e^{-x} - 2 x e^{-x}}{2 e^{-2x}}} \\
                                  &= \frac{2 e^{0}}{2 e^{0}} \\
                                  &= 1
        \end{align*}
        As $x \to \infty$, $2xe^{-x} \to 0$ and $1 - e^{-2x} \to 1$, therefore $\kappa \to 0$.
        As $x \to -\infty$, $1 - e^{-2x} > 2xe^{-x}$, so $\kappa \to 0$.
        In fact $\kappa \le 1$ for all $x$, therefore this problem is not
        ill-conditioned.

    \item % #2 Done
        Determine whether the calculation $f(x, y) = (1 + x)y^2$ is backward
        stable by the alogirithm
        \[
            \tilde{f}(x, y) = \br{1 \oplus fl(x)} \otimes \br{fl(y) \otimes fl(y)}
        \]

        The algorithm $\tilde{f}$ is backward stable if there exists
        $\tilde{\v{x}} = (\tilde{x}, \tilde{y})$ such that
        $\tilde{f}(x, y) = f(\tilde{x}, \tilde{y})$ and
        $\frac{\norm{\v{x} - \tilde{\v{x}}}}{\norm{\v{x}}} = O(\epsilon_{machine})$
        for all $\v{x}$.

        \begin{align*}
            \tilde{f}(x, y) &= \br{1 \oplus fl(x)} \otimes \br{fl(y) \otimes fl(y)} \\
            &= \br{1 \oplus x(1 + \epsilon_1)} \otimes \br{y(1 + \epsilon_2) \otimes y(1 + \epsilon_3)} \\
            &= \br{1 + x(1 + \epsilon_1)}(1 + \epsilon_4) \otimes \br{y(1 + \epsilon_2) \times y(1 + \epsilon_3)}(1 + \epsilon_5) \\
            &= \br{1 + x(1 + \epsilon_1)}(1 + \epsilon_4) \times \br{y(1 + \epsilon_2) \times y(1 + \epsilon_3)}(1 + \epsilon_5)(1 + \epsilon_6) \\
            &= \br{1 + x(1 + \epsilon_1)} y^2 (1 + \epsilon_7)
            \intertext{where $\epsilon_7 = O(\epsilon_{machine})$}
            \tilde{f}(x, y) &= \br{1 + x(1 + \epsilon_1)} y^2 (1 + \epsilon_7) \\
                            &= \br{1 + x(1 + \epsilon_1)} \p{y \sqrt{1 + \epsilon_7}}^2 \\
                            &= \br{1 + x(1 + \epsilon_1)} \p{y (1 + \epsilon_8)}^2 \\
                            &= f(x(1 + \epsilon_1), y (1 + \epsilon_8))
        \end{align*}
        Therefore $\tilde{x} = x(1 + \epsilon_1)$ and
        $\tilde{y} = y (1 + \epsilon_8)$.
        This does satisfy $\frac{\norm{\v{x} - \tilde{\v{x}}}}{\norm{\v{x}}} = O(\epsilon_{machine})$,
        because $\norm{\v{x} - \tilde{\v{x}}} = \sqrt{\epsilon_1^2 + \epsilon_8^2} = O(\epsilon_{machine})$.
        So this algorithm is backward stable.

    \item % #3 Done
        \begin{enumerate}
            \item[(a)] % Done
                Compute the LU factorization $A = LU$, of
                \[
                    A =
                    \begin{bmatrix}
                        1 & 2 & 4 \\
                        2 & 3 & 4 \\
                        2 & 5 & 6
                    \end{bmatrix}
                \]
                Use the factorization to solve the system $A\v{x} = \v{b}$
                where $\v{b} = \br{-1, 1, 1}^T$.

                Following algorithm 20.1, the $LU$ factorization can be found
                as follows.
                \begin{align*}
                    L &=
                    \begin{bmatrix}
                        1 & 0 & 0 \\
                        0 & 1 & 0 \\
                        0 & 0 & 1
                    \end{bmatrix}
                    U =
                    \begin{bmatrix}
                        1 & 2 & 4 \\
                        2 & 3 & 4 \\
                        2 & 5 & 6
                    \end{bmatrix} \\
                    k &= 1 \\
                    L &=
                    \begin{bmatrix}
                        1 & 0 & 0 \\
                        2 & 1 & 0 \\
                        2 & 0 & 1
                    \end{bmatrix}
                    U =
                    \begin{bmatrix}
                        1 & 2 & 4 \\
                        0 & -1 & -4 \\
                        0 & 1 & -2
                    \end{bmatrix} \\
                    k &= 2 \\
                    L &=
                    \begin{bmatrix}
                        1 & 0 & 0 \\
                        2 & 1 & 0 \\
                        2 & -1 & 1
                    \end{bmatrix}
                    U =
                    \begin{bmatrix}
                        1 & 2 & 4 \\
                        0 & -1 & -4 \\
                        0 & 0 & -6
                    \end{bmatrix} \\
                \end{align*}
                Therefore the $LU$ factorization of $A$ is
                \begin{align*}
                    A = LU = 
                    \begin{bmatrix}
                        1 & 0 & 0 \\
                        2 & 1 & 0 \\
                        2 & -1 & 1
                    \end{bmatrix}
                    \begin{bmatrix}
                        1 & 2 & 4 \\
                        0 & -1 & -4 \\
                        0 & 0 & -6
                    \end{bmatrix}
                \end{align*}

                Now this system can be solved using forward and backward
                substitution.
                Initially we will solve the system $L\v{y} = \v{b}$ where
                $\v{y} = U\v{x}$ by forward substitution.
                \begin{align*}
                    L\v{y} &= \v{b} \\
                    \begin{bmatrix}
                        1 & 0 & 0 \\
                        2 & 1 & 0 \\
                        2 & -1 & 1
                    \end{bmatrix}
                    \begin{bmatrix}
                        y_1 \\
                        y_2 \\
                        y_3
                    \end{bmatrix}
                    &=
                    \begin{bmatrix}
                        -1 \\
                        1 \\
                        1
                    \end{bmatrix} \\
                    \begin{bmatrix}
                        1 & 0 & 0 \\
                        0 & 1 & 0 \\
                        2 & -1 & 1
                    \end{bmatrix}
                    \begin{bmatrix}
                        y_1 \\
                        y_2 \\
                        y_3
                    \end{bmatrix}
                    &=
                    \begin{bmatrix}
                        -1 \\
                        3 \\
                        1
                    \end{bmatrix} \\
                    \begin{bmatrix}
                        1 & 0 & 0 \\
                        0 & 1 & 0 \\
                        0 & 0 & 1
                    \end{bmatrix}
                    \begin{bmatrix}
                        y_1 \\
                        y_2 \\
                        y_3
                    \end{bmatrix}
                    &=
                    \begin{bmatrix}
                        -1 \\
                        3 \\
                        6
                    \end{bmatrix}
                    \intertext{Therefore}
                    \begin{bmatrix}
                        y_1 \\
                        y_2 \\
                        y_3
                    \end{bmatrix} &=
                    \begin{bmatrix}
                        -1 \\
                        3 \\
                        6
                    \end{bmatrix}
                \end{align*}

                Now we can solve the system $U\v{x} = \v{y}$ by backward
                substitution.
                \begin{align*}
                    U\v{x} &= \v{y} \\
                    \begin{bmatrix}
                        1 & 2 & 4 \\
                        0 & -1 & -4 \\
                        0 & 0 & -6
                    \end{bmatrix}
                    \begin{bmatrix}
                        x_1 \\
                        x_2 \\
                        x_3
                    \end{bmatrix}
                    &=
                    \begin{bmatrix}
                        -1 \\
                        3 \\
                        6
                    \end{bmatrix} \\
                    \begin{bmatrix}
                        1 & 2 & 4 \\
                        0 & -1 & -4 \\
                        0 & 0 & 1
                    \end{bmatrix}
                    \begin{bmatrix}
                        x_1 \\
                        x_2 \\
                        x_3
                    \end{bmatrix}
                    &=
                    \begin{bmatrix}
                        -1 \\
                        3 \\
                        -1
                    \end{bmatrix} \\
                    \begin{bmatrix}
                        1 & 2 & 4 \\
                        0 & 1 & 0 \\
                        0 & 0 & 1
                    \end{bmatrix}
                    \begin{bmatrix}
                        x_1 \\
                        x_2 \\
                        x_3
                    \end{bmatrix}
                    &=
                    \begin{bmatrix}
                        -1 \\
                        1 \\
                        -1
                    \end{bmatrix} \\
                    \begin{bmatrix}
                        1 & 0 & 0 \\
                        0 & 1 & 0 \\
                        0 & 0 & 1
                    \end{bmatrix}
                    \begin{bmatrix}
                        x_1 \\
                        x_2 \\
                        x_3
                    \end{bmatrix}
                    &=
                    \begin{bmatrix}
                        1 \\
                        1 \\
                        -1
                    \end{bmatrix}
                    \intertext{Therefore}
                    \begin{bmatrix}
                        x_1 \\
                        x_2 \\
                        x_3
                    \end{bmatrix}
                    &=
                    \begin{bmatrix}
                        1 \\
                        1 \\
                        -1
                    \end{bmatrix}
                \end{align*}
                This is also the solution to the system $A\v{x} = \v{b}$.

            \item[(b)] % Done
                Solve the system $A\v{x} = \v{b}$ by LU factorization with
                partial pivoting

                The $LU$ factorization using partial pivoting can be found by
                following algorithm 21.1.
                \begin{align*}
                    L &=
                    \begin{bmatrix}
                        1 & 0 & 0 \\
                        0 & 1 & 0 \\
                        0 & 0 & 1
                    \end{bmatrix}
                    U =
                    \begin{bmatrix}
                        1 & 2 & 4 \\
                        2 & 3 & 4 \\
                        2 & 5 & 6
                    \end{bmatrix}
                    P =
                    \begin{bmatrix}
                        1 & 0 & 0 \\
                        0 & 1 & 0 \\
                        0 & 0 & 1
                    \end{bmatrix} \\
                    k = 1 \\
                    L &=
                    \begin{bmatrix}
                        1 & 0 & 0 \\
                        1/2 & 1 & 0 \\
                        1 & 0 & 1
                    \end{bmatrix}
                    U =
                    \begin{bmatrix}
                        2 & 3 & 4 \\
                        0 & 1/2 & 2 \\
                        0 & 2 & 2
                    \end{bmatrix}
                    P =
                    \begin{bmatrix}
                        0 & 1 & 0 \\
                        1 & 0 & 0 \\
                        0 & 0 & 1
                    \end{bmatrix} \\
                    k = 2 \\
                    L &=
                    \begin{bmatrix}
                        1 & 0 & 0 \\
                        1 & 1 & 0 \\
                        1/2 & 1/4 & 1
                    \end{bmatrix}
                    U =
                    \begin{bmatrix}
                        2 & 3 & 4 \\
                        0 & 2 & 2 \\
                        0 & 0 & 3/2
                    \end{bmatrix}
                    P =
                    \begin{bmatrix}
                        0 & 1 & 0 \\
                        0 & 0 & 1 \\
                        1 & 0 & 0
                    \end{bmatrix} \\
                \end{align*}

                Now that we have found this $LU$ factorization with pivoting we
                can solve the system $A\v{x} = \v{b}$ or the equivalent system
                $PA\v{x} = P\v{b}$, using forward and backward substitution.
                \begin{align*}
                    A\v{x} &= \v{b} \\
                    PA\v{x} &= P\v{b} \\
                    LU\v{x} &= P\v{b}
                    \intertext{Let $\v{y} = U\v{x}$ and solve this system with forward substitution.}
                    L\v{y} &= P\v{b} \\
                    \begin{bmatrix}
                        1 & 0 & 0 \\
                        1 & 1 & 0 \\
                        1/2 & 1/4 & 1
                    \end{bmatrix}
                    \begin{bmatrix}
                        y_1 \\
                        y_2 \\
                        y_3
                    \end{bmatrix}
                    &=
                    \begin{bmatrix}
                        0 & 1 & 0 \\
                        0 & 0 & 1 \\
                        1 & 0 & 0
                    \end{bmatrix}
                    \begin{bmatrix}
                        -1 \\
                        1 \\
                        1
                    \end{bmatrix}
                    =
                    \begin{bmatrix}
                        1 \\
                        1 \\
                        -1
                    \end{bmatrix} \\
                    \begin{bmatrix}
                        1 & 0 & 0 \\
                        0 & 1 & 0 \\
                        1/2 & 1/4 & 1
                    \end{bmatrix}
                    \begin{bmatrix}
                        y_1 \\
                        y_2 \\
                        y_3
                    \end{bmatrix}
                    &=
                    \begin{bmatrix}
                        1 \\
                        0 \\
                        -1
                    \end{bmatrix} \\
                    \begin{bmatrix}
                        1 & 0 & 0 \\
                        0 & 1 & 0 \\
                        0 & 0 & 1
                    \end{bmatrix}
                    \begin{bmatrix}
                        y_1 \\
                        y_2 \\
                        y_3
                    \end{bmatrix}
                    &=
                    \begin{bmatrix}
                        1 \\
                        0 \\
                        -3/2
                    \end{bmatrix}
                    \intertext{Therefore}
                    \begin{bmatrix}
                        y_1 \\
                        y_2 \\
                        y_3
                    \end{bmatrix}
                    &=
                    \begin{bmatrix}
                        1 \\
                        0 \\
                        -3/2
                    \end{bmatrix}
                \end{align*}

                Now the system $U\v{x} = \v{y}$ can be solved by backward
                substitution.
                \begin{align*}
                    \begin{bmatrix}
                        2 & 3 & 4 \\
                        0 & 2 & 2 \\
                        0 & 0 & 3/2
                    \end{bmatrix}
                    \begin{bmatrix}
                        x_1 \\
                        x_2 \\
                        x_3
                    \end{bmatrix}
                    &= 
                    \begin{bmatrix}
                        1 \\
                        0 \\
                        -3/2
                    \end{bmatrix} \\
                    \begin{bmatrix}
                        2 & 3 & 4 \\
                        0 & 2 & 2 \\
                        0 & 0 & 1
                    \end{bmatrix}
                    \begin{bmatrix}
                        x_1 \\
                        x_2 \\
                        x_3
                    \end{bmatrix}
                    &= 
                    \begin{bmatrix}
                        1 \\
                        0 \\
                        -1
                    \end{bmatrix} \\
                    \begin{bmatrix}
                        2 & 3 & 4 \\
                        0 & 1 & 0 \\
                        0 & 0 & 1
                    \end{bmatrix}
                    \begin{bmatrix}
                        x_1 \\
                        x_2 \\
                        x_3
                    \end{bmatrix}
                    &= 
                    \begin{bmatrix}
                        1 \\
                        1 \\
                        -1
                    \end{bmatrix} \\
                    \begin{bmatrix}
                        1 & 0 & 0 \\
                        0 & 1 & 0 \\
                        0 & 0 & 1
                    \end{bmatrix}
                    \begin{bmatrix}
                        x_1 \\
                        x_2 \\
                        x_3
                    \end{bmatrix}
                    &= 
                    \begin{bmatrix}
                        1 \\
                        1 \\
                        -1
                    \end{bmatrix} \\
                \end{align*}
                This is also the solution to $A\v{x} = \v{b}$ and it is
                equivalent to the solution found in part (a).
        \end{enumerate}

    \item % #4
        Let $A \in \CC^{m \times m}$ be nonsingular.
        Show that $A$ has an LU factorization if and only if for each $k$, such
        that $1 \le k \le m$, the upper left $(k \times k)$ block $A(1:k, 1:k)$
        of $A$ is nonsingular.
        Show that this LU factorization is unique.

        \begin{proof}
            Let $A \in \CC^{m \times m}$ be nonsingular.
            Suppose that $A$ has an $LU$ factorization.
            It is known that $\det(A) = \det(L)\times\det(U)$.
            Since $L$ and $U$ are triangular the deteminants of $L$ and $U$ are
            the product of the entries along the diagonal.
            Since the diagonal of $L$ is all ones, $\det(L) = 1$.
            Therefore $\det(A) = \det(U)$.
            Since $A$ is nonsingular, $\det(A) \neq 0$.
            This implies that all of the diagonal entries of $U$ are nonzero.
            Also note that $A(1:k, 1:k) = L(1:k, 1:k) U(1:k, 1:k)$ for $1 \le k \le m$, so
            $\det(A(1:k, 1:k)) = \det(L(1:k, 1:k)) \det(U(1:k, 1:k))$.
            As before $\det(L(1:k, 1:k)) = 1$.
            Since all of the diagonal entries of $U$ are nonzero
            $\det(U(1:k,1:k)) \neq 0$.
            This implies that $\det(A(1:k, 1:k)) \neq 0$ and thus $A(1:k, 1:k)$
            is nonsingular.

            Now suppose that $A(1:k, 1:k)$ is nonsingular for $1 \le k \le m$.

        \end{proof}

    \item % #5
        Rank Deficient Least Squares Problem:

    \item % #6 Done
        Consider the matrix
        \[
            A =
            \begin{bmatrix}
                1 & 2 \\
                0 & 1 \\
                1 & 0 \\
            \end{bmatrix}
        \]
        \begin{enumerate}
            \item[(a)] % Done
                Using any method you like, determine reduced and full $QR$
                factorizations.

                I will use classical Gram-Schmidt
                \begin{align*}
                    r_{11} &= \norm{a_1} = \sqrt{2} \\
                    q_1 &= a_1/r_{11} \\
                    &= 
                    \begin{bmatrix}
                        1/\sqrt{2} \\
                        0 \\
                        1/\sqrt{2}
                    \end{bmatrix} \\
                    v_2 &= a_2 \\
                    &= 
                    \begin{bmatrix}
                        2 \\
                        1 \\
                        0
                    \end{bmatrix} \\
                    r_{12} &= q_1^* a_2 \\
                    &= 2/\sqrt{2} = \sqrt{2}
                    v_2 &= v_2 - r_{12}q_1 \\
                    &= 
                    \begin{bmatrix}
                        1 \\
                        1 \\
                        -1
                    \end{bmatrix} \\
                    r_{22} &= \norm{v_2} = \sqrt{3} \\
                    q_2 &= v_2/r_{22} \\
                    &=
                    \begin{bmatrix}
                        1/\sqrt{3} \\
                        1/\sqrt{3} \\
                        -1/\sqrt{3}
                    \end{bmatrix}
                \end{align*}
                Therefore the reduced $QR$ factorization of $A$ is
                \begin{align*}
                    A &= \hat{Q} \hat{R} \\
                    &=
                    \begin{bmatrix}
                        1/\sqrt{2} & 1/\sqrt{3} \\
                        0 & 1/\sqrt{3} \\
                        1/\sqrt{2} & -1/\sqrt{3}
                    \end{bmatrix}
                    \begin{bmatrix}
                        \sqrt{2} & \sqrt{2} \\
                        0 & \sqrt{3}
                    \end{bmatrix} \\
                \end{align*}
                The full $QR$ factorization of $A$ can be found by adding a
                column to $\hat{Q}$ to make it unitary and adding a row of
                zeroes to $\hat{R}$.
                The vector $[-1, 2, 1]$ is orthogonal to both $q_1$ and $q_2$.
                Normalizing this vector results in the last column of $Q$.
                Therefore the full $QR$ factorization of $A$ is
                \begin{align*}
                    A &= QR \\
                      &=
                    \begin{bmatrix}
                        1/\sqrt{2} & 1/\sqrt{3} & -1/\sqrt{6} \\
                        0 & 1/\sqrt{3} & 2/\sqrt{6} \\
                        1/\sqrt{2} & -1/\sqrt{3} & 1/\sqrt{6}
                    \end{bmatrix}
                    \begin{bmatrix}
                        \sqrt{2} & \sqrt{2} \\
                        0 & \sqrt{3} \\
                        0 & 0
                    \end{bmatrix} \\
                \end{align*}

            \item[(b)] % Done
                Use the $QR$ factorization to solve the linear least square problem
                \[
                    \min*_{\v{x}}\norm[2]{A\v{x} - \v{b}}^2
                \]
                with $\v{b} = \br{1 1 0}^T$.

                The solution to this problem is equivalent to the solution to
                the following system $\hat{R}\v{x} = \hat{Q}^*\v{b}$.
                This system can be solved using backsubstitution.
                \begin{align*}
                    \begin{bmatrix}
                        \sqrt{2} & \sqrt{2} \\
                        0 & \sqrt{3}
                    \end{bmatrix}
                    \begin{bmatrix}
                        x_1 \\
                        x_2
                    \end{bmatrix} &=
                    \begin{bmatrix}
                        1/\sqrt{2} & 0 & 1\sqrt{2} \\
                        1/\sqrt{3} & 1/\sqrt{3} & -1/\sqrt{3}
                    \end{bmatrix}
                    \begin{bmatrix}
                        1 \\
                        1 \\
                        0
                    \end{bmatrix} \\
                    \begin{bmatrix}
                        \sqrt{2} & \sqrt{2} \\
                        0 & \sqrt{3}
                    \end{bmatrix}
                    \begin{bmatrix}
                        x_1 \\
                        x_2
                    \end{bmatrix} &=
                    \begin{bmatrix}
                        1/\sqrt{2} \\
                        2/\sqrt{3}
                    \end{bmatrix} \\
                    \begin{bmatrix}
                        \sqrt{2} & \sqrt{2} \\
                        0 & 1
                    \end{bmatrix}
                    \begin{bmatrix}
                        x_1 \\
                        x_2
                    \end{bmatrix} &=
                    \begin{bmatrix}
                        1/\sqrt{2} \\
                        2/3
                    \end{bmatrix} \\
                    \begin{bmatrix}
                        1 & 0 \\
                        0 & 1
                    \end{bmatrix}
                    \begin{bmatrix}
                        x_1 \\
                        x_2
                    \end{bmatrix} &=
                    \begin{bmatrix}
                        -1/6 \\
                        2/3
                    \end{bmatrix} \\
                \end{align*}
                Therefore the solution is $x = [-1/6, 2/3]^T$.

            \item[(c)] %Done
                Use the $QR$ factorization to solve the linear least squares
                problem
                \[
                    \min*_{\v{x}}\norm[2]{A\v{x} - \v{b}}^2
                \]
                with matrix $A \in \RR^{m \times n}$ with rank $n$ and
                $\v{b} \in \RR^m$.

                It is known that the solution to this problem is the vector
                $\v{x}$ such that $A\v{x}$ is the projection of $\v{b}$ onto
                the range of $A$.
                Using the reduced $QR$ factorization the orthogonal projector onto
                $A$ is $\hat{Q}\hat{Q}^*$.
                Thus the orthogonal projection of $\v{b}$ onto the range of $A$
                is $\hat{Q}\hat{Q}^* \v{b}$.
                The solution is therefore the solution to the system
                $A\v{x} = \hat{Q}\hat{Q}^* \v{b}$.
                Replacing $A$ with its reduced $QR$ decomposition and left
                multiplying by $\hat{Q}^*$ results in the system
                $\hat{R}\v{x} = \hat{Q}^*\v{b}$.
                Since $\hat{R}$ is upper triangular this system can be solved
                using back-substitution.
                The solution to this system is the solution to the linear
                least squares problem.
        \end{enumerate}

    \item % #7
        Consider the least-square problem $\min*_{\v{x}}\norm[2]{A\v{x} - b}$,
        where $A$ is the first 5 columns of the $6 \times 6$ inverse Hilbert
        matrix and
        \[
            b =
            \begin{bmatrix}
                463 \\
                -13860 \\
                97020 \\
                -258720 \\
                291060 \\
                -116424
            \end{bmatrix}
        \]
        \begin{enumerate}
            \item[(a)]
                What are the four conditioning numbers (Theorem 18.1) of the
                problem?

                The following script computes the 4 condition numbers.
                \lstinputlisting[language=Matlab, lastline=14]{H03.m}
                \begin{verbatim}
                    condby =

                        1

                    condbx =

                        1.8263e+05

                    condAy =

                        4.6968e+06

                    condAx =

                        4.6968e+06
                \end{verbatim}

            \item[(b)]
                Use all the algorithms on Pages 138-142 to solve the problem.
                \begin{enumerate}
                    \item Householder QR
                    \item Householder QR of augmented matrix
                    \item Modified Gram-Schmidt QR
                    \item Modified Gram-Schmidt QR of augmented matrix
                    \item Normal Equation
                    \item SVD
                \end{enumerate}
                Check the accuracy of computed solutions as compared to actual
                solution, and comment on the computed solutions and algorithms
                used.

                \lstinputlisting[language=Matlab, firstline=16]{H03.m}
                \begin{verbatim}
ans = 

                                                    Error   
                                                    __________

    Householder QR                                  8.3668e-11
    Householder QR of augmented matrix              6.0204e-11
    Modified Gram-Schmidt QR                        4.2067e-06
    Modified Gram-Schmidt QR of augmented matrix     1.253e-12
    Normal Equations                                1.0259e-05
    SVD                                             1.2091e-10
                \end{verbatim}

        \end{enumerate}

\end{enumerate}
\end{document}
