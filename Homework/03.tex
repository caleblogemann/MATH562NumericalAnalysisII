\documentclass[11pt]{article}
\usepackage[letterpaper]{geometry}
\usepackage{MATH562}

\begin{document}
\noindent \textbf{\Large{Caleb Logemann \\
MATH 562 Numerical Analysis II \\
Homework 3
}}

%\lstinputlisting[language=Matlab]{H01_23.m}
\begin{enumerate}
    \item % #1 Done
        Determine the relative condition number for the following problem.
        Are there values of $x$ for which the problem is ill-conditioned?
        Justify your answer.
        \[
            f(x) = \frac{1 - e^{-x}}{1 + e^{-x}}
        \]

        Since $f$ is differentiable the relative condition number of $f$ is given by
        $\kappa = \frac{\abs{f'(x)}}{\abs{f(x)}/\abs{x}}$.
        For this problem
        \begin{align*}
            f'(x) &= \frac{\p{1 + e^{-x}}e^{-x} - \p{1 - e^{-x}}\p{-e^{-x}}}{\p{1 + e^{-x}}^2} \\
                  &= \frac{e^{-x} + e^{-2x} + e^{-x} - e^{-2x}}{\p{1 + e^{-x}}^2} \\ 
                  &= \frac{2e^{-x}}{\p{1 + e^{-x}}^2} \\ 
        \end{align*}
        Thus the relative condition number for this problem is
        \begin{align*}
            \kappa &= \frac{\abs{f'(x)}}{\abs{f(x)}/\abs{x}} \\
                   &= \abs{\frac{2xe^{-x}}{\p{1 + e^{-x}}^2} / \frac{1 - e^{-x}}{1 + e^{-x}}} \\
                   &= \abs{\frac{2xe^{-x}}{\p{1 + e^{-x}}^2} \times \frac{1 + e^{-x}}{1 - e^{-x}}} \\
                   &= \abs{\frac{2xe^{-x}}{\p{1 + e^{-x}}} \times \frac{1}{1 - e^{-x}}} \\
                   &= \abs{\frac{2xe^{-x}}{\p{1 - e^{-2x}}}} \\
        \end{align*}

        This problem is not ill-conditioned because for any $x$ this relitive
        condition number is small.
        At $x = 0$, this condition number is undefined, but L'Hopital's rule
        shows that the limit is equal to $1$.
        \begin{align*}
            \lim{x \to 0}{\kappa} &= \lim{x \to 0}{\frac{2 e^{-x} - 2 x e^{-x}}{2 e^{-2x}}} \\
                                  &= \frac{2 e^{0}}{2 e^{0}} \\
                                  &= 1
        \end{align*}
        As $x \to \infty$, $2xe^{-x} \to 0$ and $1 - e^{-2x} \to 1$, therefore $\kappa \to 0$.
        As $x \to -\infty$, $1 - e^{-2x} > 2xe^{-x}$, so $\kappa \to 0$.
        In fact $\kappa \le 1$ for all $x$, therefore this problem is not
        ill-conditioned.

    \item % #2 Done
        Determine whether the calculation $f(x, y) = (1 + x)y^2$ is backward
        stable by the alogirithm
        \[
            \tilde{f}(x, y) = \br{1 \oplus fl(x)} \otimes \br{fl(y) \otimes fl(y)}
        \]

        The algorithm $\tilde{f}$ is backward stable if there exists
        $\tilde{\v{x}} = (\tilde{x}, \tilde{y})$ such that
        $\tilde{f}(x, y) = f(\tilde{x}, \tilde{y})$ and
        $\frac{\norm{\v{x} - \tilde{\v{x}}}}{\norm{\v{x}}} = O(\epsilon_{machine})$
        for all $\v{x}$.

        \begin{align*}
            \tilde{f}(x, y) &= \br{1 \oplus fl(x)} \otimes \br{fl(y) \otimes fl(y)} \\
            &= \br{1 \oplus x(1 + \epsilon_1)} \otimes \br{y(1 + \epsilon_2) \otimes y(1 + \epsilon_3)} \\
            &= \br{1 + x(1 + \epsilon_1)}(1 + \epsilon_4) \otimes \br{y(1 + \epsilon_2) \times y(1 + \epsilon_3)}(1 + \epsilon_5) \\
            &= \br{1 + x(1 + \epsilon_1)}(1 + \epsilon_4) \times \br{y(1 + \epsilon_2) \times y(1 + \epsilon_3)}(1 + \epsilon_5)(1 + \epsilon_6) \\
            &= \br{1 + x(1 + \epsilon_1)} y^2 (1 + \epsilon_7)
            \intertext{where $\epsilon_7 = O(\epsilon_{machine})$}
            \tilde{f}(x, y) &= \br{1 + x(1 + \epsilon_1)} y^2 (1 + \epsilon_7) \\
                            &= \br{1 + x(1 + \epsilon_1)} \p{y \sqrt{1 + \epsilon_7}}^2 \\
                            &= \br{1 + x(1 + \epsilon_1)} \p{y (1 + \epsilon_8)}^2 \\
                            &= f(x(1 + \epsilon_1), y (1 + \epsilon_8))
        \end{align*}
        Therefore $\tilde{x} = x(1 + \epsilon_1)$ and
        $\tilde{y} = y (1 + \epsilon_8)$.
        This does satisfy $\frac{\norm{\v{x} - \tilde{\v{x}}}}{\norm{\v{x}}} = O(\epsilon_{machine})$,
        because $\norm{\v{x} - \tilde{\v{x}}} = \sqrt{\epsilon_1^2 + \epsilon_8^2} = O(\epsilon_{machine})$.
        So this algorithm is backward stable.

    \item % #3 Done
        \begin{enumerate}
            \item[(a)] % Done
                Compute the LU factorization $A = LU$, of
                \[
                    A =
                    \begin{bmatrix}
                        1 & 2 & 4 \\
                        2 & 3 & 4 \\
                        2 & 5 & 6
                    \end{bmatrix}
                \]
                Use the factorization to solve the system $A\v{x} = \v{b}$
                where $\v{b} = \br{-1, 1, 1}^T$.

                Following algorithm 20.1, the $LU$ factorization can be found
                as follows.
                \begin{align*}
                    L &=
                    \begin{bmatrix}
                        1 & 0 & 0 \\
                        0 & 1 & 0 \\
                        0 & 0 & 1
                    \end{bmatrix}
                    U =
                    \begin{bmatrix}
                        1 & 2 & 4 \\
                        2 & 3 & 4 \\
                        2 & 5 & 6
                    \end{bmatrix} \\
                    k &= 1 \\
                    L &=
                    \begin{bmatrix}
                        1 & 0 & 0 \\
                        2 & 1 & 0 \\
                        2 & 0 & 1
                    \end{bmatrix}
                    U =
                    \begin{bmatrix}
                        1 & 2 & 4 \\
                        0 & -1 & -4 \\
                        0 & 1 & -2
                    \end{bmatrix} \\
                    k &= 2 \\
                    L &=
                    \begin{bmatrix}
                        1 & 0 & 0 \\
                        2 & 1 & 0 \\
                        2 & -1 & 1
                    \end{bmatrix}
                    U =
                    \begin{bmatrix}
                        1 & 2 & 4 \\
                        0 & -1 & -4 \\
                        0 & 0 & -6
                    \end{bmatrix} \\
                \end{align*}
                Therefore the $LU$ factorization of $A$ is
                \begin{align*}
                    A = LU = 
                    \begin{bmatrix}
                        1 & 0 & 0 \\
                        2 & 1 & 0 \\
                        2 & -1 & 1
                    \end{bmatrix}
                    \begin{bmatrix}
                        1 & 2 & 4 \\
                        0 & -1 & -4 \\
                        0 & 0 & -6
                    \end{bmatrix}
                \end{align*}

                Now this system can be solved using forward and backward
                substitution.
                Initially we will solve the system $L\v{y} = \v{b}$ where
                $\v{y} = U\v{x}$ by forward substitution.
                \begin{align*}
                    L\v{y} &= \v{b} \\
                    \begin{bmatrix}
                        1 & 0 & 0 \\
                        2 & 1 & 0 \\
                        2 & -1 & 1
                    \end{bmatrix}
                    \begin{bmatrix}
                        y_1 \\
                        y_2 \\
                        y_3
                    \end{bmatrix}
                    &=
                    \begin{bmatrix}
                        -1 \\
                        1 \\
                        1
                    \end{bmatrix} \\
                    \begin{bmatrix}
                        1 & 0 & 0 \\
                        0 & 1 & 0 \\
                        2 & -1 & 1
                    \end{bmatrix}
                    \begin{bmatrix}
                        y_1 \\
                        y_2 \\
                        y_3
                    \end{bmatrix}
                    &=
                    \begin{bmatrix}
                        -1 \\
                        3 \\
                        1
                    \end{bmatrix} \\
                    \begin{bmatrix}
                        1 & 0 & 0 \\
                        0 & 1 & 0 \\
                        0 & 0 & 1
                    \end{bmatrix}
                    \begin{bmatrix}
                        y_1 \\
                        y_2 \\
                        y_3
                    \end{bmatrix}
                    &=
                    \begin{bmatrix}
                        -1 \\
                        3 \\
                        6
                    \end{bmatrix}
                    \intertext{Therefore}
                    \begin{bmatrix}
                        y_1 \\
                        y_2 \\
                        y_3
                    \end{bmatrix} &=
                    \begin{bmatrix}
                        -1 \\
                        3 \\
                        6
                    \end{bmatrix}
                \end{align*}

                Now we can solve the system $U\v{x} = \v{y}$ by backward
                substitution.
                \begin{align*}
                    U\v{x} &= \v{y} \\
                    \begin{bmatrix}
                        1 & 2 & 4 \\
                        0 & -1 & -4 \\
                        0 & 0 & -6
                    \end{bmatrix}
                    \begin{bmatrix}
                        x_1 \\
                        x_2 \\
                        x_3
                    \end{bmatrix}
                    &=
                    \begin{bmatrix}
                        -1 \\
                        3 \\
                        6
                    \end{bmatrix} \\
                    \begin{bmatrix}
                        1 & 2 & 4 \\
                        0 & -1 & -4 \\
                        0 & 0 & 1
                    \end{bmatrix}
                    \begin{bmatrix}
                        x_1 \\
                        x_2 \\
                        x_3
                    \end{bmatrix}
                    &=
                    \begin{bmatrix}
                        -1 \\
                        3 \\
                        -1
                    \end{bmatrix} \\
                    \begin{bmatrix}
                        1 & 2 & 4 \\
                        0 & 1 & 0 \\
                        0 & 0 & 1
                    \end{bmatrix}
                    \begin{bmatrix}
                        x_1 \\
                        x_2 \\
                        x_3
                    \end{bmatrix}
                    &=
                    \begin{bmatrix}
                        -1 \\
                        1 \\
                        -1
                    \end{bmatrix} \\
                    \begin{bmatrix}
                        1 & 0 & 0 \\
                        0 & 1 & 0 \\
                        0 & 0 & 1
                    \end{bmatrix}
                    \begin{bmatrix}
                        x_1 \\
                        x_2 \\
                        x_3
                    \end{bmatrix}
                    &=
                    \begin{bmatrix}
                        1 \\
                        1 \\
                        -1
                    \end{bmatrix}
                    \intertext{Therefore}
                    \begin{bmatrix}
                        x_1 \\
                        x_2 \\
                        x_3
                    \end{bmatrix}
                    &=
                    \begin{bmatrix}
                        1 \\
                        1 \\
                        -1
                    \end{bmatrix}
                \end{align*}
                This is also the solution to the system $A\v{x} = \v{b}$.

            \item[(b)] % Done
                Solve the system $A\v{x} = \v{b}$ by LU factorization with
                partial pivoting

                The $LU$ factorization using partial pivoting can be found by
                following algorithm 21.1.
                \begin{align*}
                    L &=
                    \begin{bmatrix}
                        1 & 0 & 0 \\
                        0 & 1 & 0 \\
                        0 & 0 & 1
                    \end{bmatrix}
                    U =
                    \begin{bmatrix}
                        1 & 2 & 4 \\
                        2 & 3 & 4 \\
                        2 & 5 & 6
                    \end{bmatrix}
                    P =
                    \begin{bmatrix}
                        1 & 0 & 0 \\
                        0 & 1 & 0 \\
                        0 & 0 & 1
                    \end{bmatrix} \\
                    k = 1 \\
                    L &=
                    \begin{bmatrix}
                        1 & 0 & 0 \\
                        1/2 & 1 & 0 \\
                        1 & 0 & 1
                    \end{bmatrix}
                    U =
                    \begin{bmatrix}
                        2 & 3 & 4 \\
                        0 & 1/2 & 2 \\
                        0 & 2 & 2
                    \end{bmatrix}
                    P =
                    \begin{bmatrix}
                        0 & 1 & 0 \\
                        1 & 0 & 0 \\
                        0 & 0 & 1
                    \end{bmatrix} \\
                    k = 2 \\
                    L &=
                    \begin{bmatrix}
                        1 & 0 & 0 \\
                        1 & 1 & 0 \\
                        1/2 & 1/4 & 1
                    \end{bmatrix}
                    U =
                    \begin{bmatrix}
                        2 & 3 & 4 \\
                        0 & 2 & 2 \\
                        0 & 0 & 3/2
                    \end{bmatrix}
                    P =
                    \begin{bmatrix}
                        0 & 1 & 0 \\
                        0 & 0 & 1 \\
                        1 & 0 & 0
                    \end{bmatrix} \\
                \end{align*}

                Now that we have found this $LU$ factorization with pivoting we
                can solve the system $A\v{x} = \v{b}$ or the equivalent system
                $PA\v{x} = P\v{b}$, using forward and backward substitution.
                \begin{align*}
                    A\v{x} &= \v{b} \\
                    PA\v{x} &= P\v{b} \\
                    LU\v{x} &= P\v{b}
                    \intertext{Let $\v{y} = U\v{x}$ and solve this system with forward substitution.}
                    L\v{y} &= P\v{b} \\
                    \begin{bmatrix}
                        1 & 0 & 0 \\
                        1 & 1 & 0 \\
                        1/2 & 1/4 & 1
                    \end{bmatrix}
                    \begin{bmatrix}
                        y_1 \\
                        y_2 \\
                        y_3
                    \end{bmatrix}
                    &=
                    \begin{bmatrix}
                        0 & 1 & 0 \\
                        0 & 0 & 1 \\
                        1 & 0 & 0
                    \end{bmatrix}
                    \begin{bmatrix}
                        -1 \\
                        1 \\
                        1
                    \end{bmatrix}
                    =
                    \begin{bmatrix}
                        1 \\
                        1 \\
                        -1
                    \end{bmatrix} \\
                    \begin{bmatrix}
                        1 & 0 & 0 \\
                        0 & 1 & 0 \\
                        1/2 & 1/4 & 1
                    \end{bmatrix}
                    \begin{bmatrix}
                        y_1 \\
                        y_2 \\
                        y_3
                    \end{bmatrix}
                    &=
                    \begin{bmatrix}
                        1 \\
                        0 \\
                        -1
                    \end{bmatrix} \\
                    \begin{bmatrix}
                        1 & 0 & 0 \\
                        0 & 1 & 0 \\
                        0 & 0 & 1
                    \end{bmatrix}
                    \begin{bmatrix}
                        y_1 \\
                        y_2 \\
                        y_3
                    \end{bmatrix}
                    &=
                    \begin{bmatrix}
                        1 \\
                        0 \\
                        -3/2
                    \end{bmatrix}
                    \intertext{Therefore}
                    \begin{bmatrix}
                        y_1 \\
                        y_2 \\
                        y_3
                    \end{bmatrix}
                    &=
                    \begin{bmatrix}
                        1 \\
                        0 \\
                        -3/2
                    \end{bmatrix}
                \end{align*}

                Now the system $U\v{x} = \v{y}$ can be solved by backward
                substitution.
                \begin{align*}
                    \begin{bmatrix}
                        2 & 3 & 4 \\
                        0 & 2 & 2 \\
                        0 & 0 & 3/2
                    \end{bmatrix}
                    \begin{bmatrix}
                        x_1 \\
                        x_2 \\
                        x_3
                    \end{bmatrix}
                    &= 
                    \begin{bmatrix}
                        1 \\
                        0 \\
                        -3/2
                    \end{bmatrix} \\
                    \begin{bmatrix}
                        2 & 3 & 4 \\
                        0 & 2 & 2 \\
                        0 & 0 & 1
                    \end{bmatrix}
                    \begin{bmatrix}
                        x_1 \\
                        x_2 \\
                        x_3
                    \end{bmatrix}
                    &= 
                    \begin{bmatrix}
                        1 \\
                        0 \\
                        -1
                    \end{bmatrix} \\
                    \begin{bmatrix}
                        2 & 3 & 4 \\
                        0 & 1 & 0 \\
                        0 & 0 & 1
                    \end{bmatrix}
                    \begin{bmatrix}
                        x_1 \\
                        x_2 \\
                        x_3
                    \end{bmatrix}
                    &= 
                    \begin{bmatrix}
                        1 \\
                        1 \\
                        -1
                    \end{bmatrix} \\
                    \begin{bmatrix}
                        1 & 0 & 0 \\
                        0 & 1 & 0 \\
                        0 & 0 & 1
                    \end{bmatrix}
                    \begin{bmatrix}
                        x_1 \\
                        x_2 \\
                        x_3
                    \end{bmatrix}
                    &= 
                    \begin{bmatrix}
                        1 \\
                        1 \\
                        -1
                    \end{bmatrix} \\
                \end{align*}
                This is also the solution to $A\v{x} = \v{b}$ and it is
                equivalent to the solution found in part (a).
        \end{enumerate}

    \item % #4 Done
        Let $A \in \CC^{m \times m}$ be nonsingular.
        Show that $A$ has an LU factorization if and only if for each $k$, such
        that $1 \le k \le m$, the upper left $(k \times k)$ block $A(1:k, 1:k)$
        of $A$ is nonsingular.
        Show that this LU factorization is unique.

        \begin{proof}
            Let $A \in \CC^{m \times m}$ be nonsingular.
            Suppose that $A$ has an $LU$ factorization.
            It is known that $\det(A) = \det(L)\times\det(U)$.
            Since $L$ and $U$ are triangular the deteminants of $L$ and $U$ are
            the product of the entries along the diagonal.
            Since the diagonal of $L$ is all ones, $\det(L) = 1$.
            Therefore $\det(A) = \det(U)$.
            Since $A$ is nonsingular, $\det(A) \neq 0$.
            This implies that all of the diagonal entries of $U$ are nonzero.
            Also note that $A(1:k, 1:k) = L(1:k, 1:k) U(1:k, 1:k)$ for $1 \le k \le m$, so
            $\det(A(1:k, 1:k)) = \det(L(1:k, 1:k)) \det(U(1:k, 1:k))$.
            As before $\det(L(1:k, 1:k)) = 1$.
            Since all of the diagonal entries of $U$ are nonzero
            $\det(U(1:k,1:k)) \neq 0$.
            This implies that $\det(A(1:k, 1:k)) \neq 0$ and thus $A(1:k, 1:k)$
            is nonsingular.

            Now suppose that $A(1:k, 1:k)$ is nonsingular for $1 \le k \le m$.
            This implies that $A(1, 1) \neq 0$, therefore Gaussian elimination
            can be applied to the first column.
            Gaussian Elimination row operations do not change the determinant of
            a matrix.
            Therefore $L_1 A$ still satisfies the property that $(L_1 A)(1:k, 1:k)$ is
            nonsingular.
            We can now conclude that $(L_1 A)(2,2) \neq 0$, because
            $(L_1 A)(1:2, 1:2)$ is upper triangular and nonsingular.
            Now Gaussian elimination can be applied to column 2.
            As is evident mathematical induction now guarantees that an
            Gaussian elimination will never fail and therefore $A$ has an $LU$
            factorization.

            To show that the $LU$ decomposition is unique assume there exists
            another $LU$ decomposition of $A$, $\hat{L}\hat{U}$.
            This implies that $LU = \hat{L}\hat{U}$, which is equivalent to
            $L^{-1}\hat{L} = U^{-1}\hat{U}$.
            $L^{-1}$ exists because $L$ has ones on the diagonal and it is lower
            triangular.
            $U^{-1}$ exists because $A$ is nonsingular and $\det{A} = \det{U}$.
            Also $U^{-1}$ is upper triangular.
            This implies that $L^{-1}\hat{L}$ is lower triangular and
            $U^{-1}\hat{U}$ is upper triangular.
            Therefore $L^{-1}\hat{L} = U^{-1}\hat{U}$ if and only if
            $L^{-1}\hat{L} = I$ and $U^{-1}\hat{U} = I$.
            Therefore $L = \hat{L}$ and $U = \hat{U}$, so the $LU$ decomposition
            is unique.
        \end{proof}

    \item % #5 Done
        Rank Deficient Least Squares Problem:
        Let $A \in \RR^{m \times n}$ with $m \ge n$, and let $r = \rank(A) < n$.
        The $SVD$ of $A$ can be written as
        \[
            A = [U_1, U_2]
            \begin{bmatrix}
                \Sigma_1 & 0 \\
                0 & 0
            \end{bmatrix}
            \br{V_1, V_2}^T = U_1 \Sigma_1 V_1^T
        \]
        where $\Sigma_1$ is $r \times r$ nonsingular and $U_1$ and $V_1$ have
        $r$ columns.
        Let $\sigma = \sigma_{min}(\Sigma_1)$, be the smallest nonzero singular
        value of $A$.
        Consider the following rank deficient least squares problem, for some
        $\v{b} \in \RR^m$.
        \[
            \min*_{\v{x} \in \RR^n} \norm[2]{A\v{x} - \v{b}}
        \]
        Show
        \begin{enumerate}
            \item[\#1] % Done
                all solutions $\v{x}$ can be written as
                $\v{x} = V_1 \Sigma_1^{-1} U_1^T \v{b} + V_2 \v{z}$
                where $\v{z}$ is an arbitrary vector.

                It is known that the solution to this problem is a vector $\v{x}$
                such that $A\v{x}$ is the projection of $\v{b}$ onto the range of
                $A$.
                Using the singular value decomposition the projector onto the
                range of $A$ is $U_1 U_1^T$.
                Therefore we need to find solutions to the system
                $A\v{x} = U_1 U_1^T \v{b}$.
                I will replace $A$ with its full SVD decomposition.
                \begin{align*}
                    A\v{x} &= U_1 U_1^T \v{b} \\
                    [U_1, U_2]
                    \begin{bmatrix}
                        \Sigma_1 & 0 \\
                        0 & 0
                    \end{bmatrix}
                    \br{V_1, V_2}^T\v{x} &= U_1 U_1^T \v{b} \\
                    \begin{bmatrix}
                        \Sigma_1 & 0 \\
                        0 & 0
                    \end{bmatrix}
                    \br{V_1, V_2}^T\v{x} &= \br{U_1, U_2}^T U_1 U_1^T \v{b} \\
                    \begin{bmatrix}
                        \Sigma_1 & 0 \\
                        0 & 0
                    \end{bmatrix}
                    \br{V_1, V_2}^T\v{x} &= I_{mr} U_1^T \v{b}
                    \intertext{Where $I_{mr}$ is the $m \times r$ matrix with
                        ones on the main diagonal.}
                    I_{rm}
                    \begin{bmatrix}
                        \Sigma_1 & 0 \\
                        0 & 0
                    \end{bmatrix}
                    \br{V_1, V_2}^T\v{x} &= U_1^T \v{b}
                    \intertext{If $m > r$ than $I_{rm} I_{mr} = I_{rr}$.}
                    \br{\Sigma_1, 0} \br{V_1, V_2}^T\v{x} &= U_1^T \v{b} \\
                    \p{\Sigma_1 V_1^T + 0 V_2^T} \v{x} &= U_1^T \v{b} \\
                    \p{V_1^T + 0 V_2^T} \v{x} &= \Sigma_1^{-1} U_1^T \v{b} \\
                    \p{I + V_1 0 V_2^T} \v{x} &= V_1 \Sigma_1^{-1} U_1^T \v{b} \\
                    \intertext{Any portion $\v{x}$ that is orthogonal to $V_2$
                        will be lost as well so an arbitrary linear combination
                        can be added to $\v{x}$.}
                    \v{x} &= V_1 \Sigma_1^{-1} U_1^T \v{b} + V_2 \v{z}
                \end{align*}

            \item[\#2] % Done
                The solution $\v{x}$ has minimal norm $\norm[2]{\v{x}}$ when
                $\v{z} = \v{0}$, and in this case
                $\norm[2]{\v{x}} \le \norm[2]{\v{b}}/\sigma$.

                From part 1 we can see that
                \begin{align*}
                    \norm[2]{\v{x}} &= \norm[2]{V_1 \Sigma_1^{-1} U_1^T \v{b} + V_2 \v{z}} \\
                    \intertext{This norm is minimized when $\v{z} = \v{0}$}
                    &= \norm[2]{V_1 \Sigma_1^{-1} U_1^T \v{b}} \\
                    &\le \norm[2]{V_1 \Sigma_1^{-1} U_1^T} \norm[2]{\v{b}}
                    \intertext{Note that $V_1 \Sigma_1^{-1} U_1^T$ is the SVD
                        of some other matrix, whose singular values are the
                        reciprocals of the singular values of $A$.
                        Therefore the 2-norm of this matrix is $1/\sigma$.}
                    &= \frac{\norm[2]{\v{b}}}{\sigma}
                \end{align*}
                Thus $\norm[2]{\v{x}} \le \norm[2]{\v{b}}/\sigma$ for the case
                when $\v{z} = \v{0}$.
        \end{enumerate}

    \item % #6 Done
        Consider the matrix
        \[
            A =
            \begin{bmatrix}
                1 & 2 \\
                0 & 1 \\
                1 & 0 \\
            \end{bmatrix}
        \]
        \begin{enumerate}
            \item[(a)] % Done
                Using any method you like, determine reduced and full $QR$
                factorizations.

                I will use classical Gram-Schmidt
                \begin{align*}
                    r_{11} &= \norm{a_1} = \sqrt{2} \\
                    q_1 &= a_1/r_{11} \\
                    &= 
                    \begin{bmatrix}
                        1/\sqrt{2} \\
                        0 \\
                        1/\sqrt{2}
                    \end{bmatrix} \\
                    v_2 &= a_2 \\
                    &= 
                    \begin{bmatrix}
                        2 \\
                        1 \\
                        0
                    \end{bmatrix} \\
                    r_{12} &= q_1^* a_2 \\
                    &= 2/\sqrt{2} = \sqrt{2} \\
                    v_2 &= v_2 - r_{12}q_1 \\
                    &= 
                    \begin{bmatrix}
                        1 \\
                        1 \\
                        -1
                    \end{bmatrix} \\
                    r_{22} &= \norm{v_2} = \sqrt{3} \\
                    q_2 &= v_2/r_{22} \\
                    &=
                    \begin{bmatrix}
                        1/\sqrt{3} \\
                        1/\sqrt{3} \\
                        -1/\sqrt{3}
                    \end{bmatrix}
                \end{align*}
                Therefore the reduced $QR$ factorization of $A$ is
                \begin{align*}
                    A &= \hat{Q} \hat{R} \\
                    &=
                    \begin{bmatrix}
                        1/\sqrt{2} & 1/\sqrt{3} \\
                        0 & 1/\sqrt{3} \\
                        1/\sqrt{2} & -1/\sqrt{3}
                    \end{bmatrix}
                    \begin{bmatrix}
                        \sqrt{2} & \sqrt{2} \\
                        0 & \sqrt{3}
                    \end{bmatrix} \\
                \end{align*}
                The full $QR$ factorization of $A$ can be found by adding a
                column to $\hat{Q}$ to make it unitary and adding a row of
                zeroes to $\hat{R}$.
                The vector $[-1, 2, 1]$ is orthogonal to both $q_1$ and $q_2$.
                Normalizing this vector results in the last column of $Q$.
                Therefore the full $QR$ factorization of $A$ is
                \begin{align*}
                    A &= QR \\
                      &=
                    \begin{bmatrix}
                        1/\sqrt{2} & 1/\sqrt{3} & -1/\sqrt{6} \\
                        0 & 1/\sqrt{3} & 2/\sqrt{6} \\
                        1/\sqrt{2} & -1/\sqrt{3} & 1/\sqrt{6}
                    \end{bmatrix}
                    \begin{bmatrix}
                        \sqrt{2} & \sqrt{2} \\
                        0 & \sqrt{3} \\
                        0 & 0
                    \end{bmatrix} \\
                \end{align*}

            \item[(b)] % Done
                Use the $QR$ factorization to solve the linear least square problem
                \[
                    \min*_{\v{x}}\norm[2]{A\v{x} - \v{b}}^2
                \]
                with $\v{b} = \br{1 1 0}^T$.

                The solution to this problem is equivalent to the solution to
                the following system $\hat{R}\v{x} = \hat{Q}^*\v{b}$.
                This system can be solved using backsubstitution.
                \begin{align*}
                    \begin{bmatrix}
                        \sqrt{2} & \sqrt{2} \\
                        0 & \sqrt{3}
                    \end{bmatrix}
                    \begin{bmatrix}
                        x_1 \\
                        x_2
                    \end{bmatrix} &=
                    \begin{bmatrix}
                        1/\sqrt{2} & 0 & 1\sqrt{2} \\
                        1/\sqrt{3} & 1/\sqrt{3} & -1/\sqrt{3}
                    \end{bmatrix}
                    \begin{bmatrix}
                        1 \\
                        1 \\
                        0
                    \end{bmatrix} \\
                    \begin{bmatrix}
                        \sqrt{2} & \sqrt{2} \\
                        0 & \sqrt{3}
                    \end{bmatrix}
                    \begin{bmatrix}
                        x_1 \\
                        x_2
                    \end{bmatrix} &=
                    \begin{bmatrix}
                        1/\sqrt{2} \\
                        2/\sqrt{3}
                    \end{bmatrix} \\
                    \begin{bmatrix}
                        \sqrt{2} & \sqrt{2} \\
                        0 & 1
                    \end{bmatrix}
                    \begin{bmatrix}
                        x_1 \\
                        x_2
                    \end{bmatrix} &=
                    \begin{bmatrix}
                        1/\sqrt{2} \\
                        2/3
                    \end{bmatrix} \\
                    \begin{bmatrix}
                        1 & 0 \\
                        0 & 1
                    \end{bmatrix}
                    \begin{bmatrix}
                        x_1 \\
                        x_2
                    \end{bmatrix} &=
                    \begin{bmatrix}
                        -1/6 \\
                        2/3
                    \end{bmatrix} \\
                \end{align*}
                Therefore the solution is $x = [-1/6, 2/3]^T$.

            \item[(c)] %Done
                Use the $QR$ factorization to solve the linear least squares
                problem
                \[
                    \min*_{\v{x}}\norm[2]{A\v{x} - \v{b}}^2
                \]
                with matrix $A \in \RR^{m \times n}$ with rank $n$ and
                $\v{b} \in \RR^m$.

                It is known that the solution to this problem is the vector
                $\v{x}$ such that $A\v{x}$ is the projection of $\v{b}$ onto
                the range of $A$.
                Using the reduced $QR$ factorization the orthogonal projector onto
                $A$ is $\hat{Q}\hat{Q}^*$.
                Thus the orthogonal projection of $\v{b}$ onto the range of $A$
                is $\hat{Q}\hat{Q}^* \v{b}$.
                The solution is therefore the solution to the system
                $A\v{x} = \hat{Q}\hat{Q}^* \v{b}$.
                Replacing $A$ with its reduced $QR$ decomposition and left
                multiplying by $\hat{Q}^*$ results in the system
                $\hat{R}\v{x} = \hat{Q}^*\v{b}$.
                Since $\hat{R}$ is upper triangular this system can be solved
                using back-substitution.
                The solution to this system is the solution to the linear
                least squares problem.
        \end{enumerate}

    \item % #7 Done
        Consider the least-square problem $\min*_{\v{x}}\norm[2]{A\v{x} - b}$,
        where $A$ is the first 5 columns of the $6 \times 6$ inverse Hilbert
        matrix and
        \[
            b =
            \begin{bmatrix}
                463 \\
                -13860 \\
                97020 \\
                -258720 \\
                291060 \\
                -116424
            \end{bmatrix}
        \]
        \begin{enumerate}
            \item[(a)]
                What are the four conditioning numbers (Theorem 18.1) of the
                problem?

                The following script computes the 4 condition numbers.
                \lstinputlisting[language=Matlab, lastline=14]{H03.m}
                \begin{verbatim}
                    condby =

                        1

                    condbx =

                        1.8263e+05

                    condAy =

                        4.6968e+06

                    condAx =

                        4.6968e+06
                \end{verbatim}

            \item[(b)]
                Use all the algorithms on Pages 138-142 to solve the problem.
                \begin{enumerate}
                    \item Householder QR
                    \item Householder QR of augmented matrix
                    \item Modified Gram-Schmidt QR
                    \item Modified Gram-Schmidt QR of augmented matrix
                    \item Normal Equation
                    \item SVD
                \end{enumerate}
                Check the accuracy of computed solutions as compared to actual
                solution, and comment on the computed solutions and algorithms
                used.

                \lstinputlisting[language=Matlab, firstline=16]{H03.m}
                \begin{verbatim}
ans = 

                                                    Error   
                                                    __________

    Householder QR                                  8.3668e-11
    Householder QR of augmented matrix              6.0204e-11
    Modified Gram-Schmidt QR                        4.2067e-06
    Modified Gram-Schmidt QR of augmented matrix     1.253e-12
    Normal Equations                                1.0259e-05
    SVD                                             1.2091e-10
                \end{verbatim}

                Note that the error for all of these methods is reasonably small.
                Some methods do have error signigicantly larger than the other
                methods.
                The modified Gram-Schmidt and Normal equations error is several
                orders of magnitude larger than the rest of the methods.
                The normal equations have much larger error because this approach is
                unstable.
                Also the accuracy of the normal equations approach is governed
                by $\kappa^2$ which will be larger than $\kappa$.
                The Modified Gram-Schmidt method is also unstable, unless the
                augmented matrix approach is taken.
                This is the reason that the error for the Modified Gram-Schmidt
                is large, while the Modified Gram-Schmidt of the augmented matrix
                produces lower error.
                The augmented matrix version is stable.
        \end{enumerate}

\end{enumerate}
\end{document}
